{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq"
      },
      "source": [
        "# Let's Train and test our model\n",
        "\n",
        "This time let's add a scheduler for out LR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjO3RK9UEnvF"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "dropout_value = 0.05\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 32 , receptive_field = 3\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 32 , receptive_field = 5\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 7\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 11\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 16 , receptive_field = 11\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 15\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 19\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), dilation=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 27\n",
        "\n",
        "         # TRANSITION BLOCK 2\n",
        "        self.convblock9 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 16 , receptive_field = 35\n",
        "\n",
        "        self.convblock10 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 16 , receptive_field = 35\n",
        "\n",
        "        self.convblock11 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 7 , receptive_field = 43\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=7)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock12 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = self.convblock9(x)\n",
        "        x = self.convblock10(x)\n",
        "        x = self.convblock11(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock12(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data augmentation and normalization for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1,cooldown=5)\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step(test_losses[epoch])\n",
        "    print(f\"Learning Rate = {optimizer.param_groups[0]['lr']}\\n\")\n",
        "    #if (test_acc[epoch] > 99.4):\n",
        "    #  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoq1J_OoY_Gv",
        "outputId": "f24fa9cf-be59-452b-b653-e85c93127621"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "CUDA Available? True\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
            "           Dropout-4           [-1, 16, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,608\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 32, 32, 32]              64\n",
            "           Dropout-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,432\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
            "          Dropout-12           [-1, 64, 16, 16]               0\n",
            "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
            "             ReLU-14          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-15          [-1, 128, 16, 16]             256\n",
            "          Dropout-16          [-1, 128, 16, 16]               0\n",
            "           Conv2d-17           [-1, 16, 16, 16]           2,048\n",
            "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
            "             ReLU-19           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-20           [-1, 32, 16, 16]              64\n",
            "          Dropout-21           [-1, 32, 16, 16]               0\n",
            "           Conv2d-22           [-1, 64, 16, 16]          18,432\n",
            "             ReLU-23           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-24           [-1, 64, 16, 16]             128\n",
            "          Dropout-25           [-1, 64, 16, 16]               0\n",
            "           Conv2d-26          [-1, 128, 14, 14]          73,728\n",
            "             ReLU-27          [-1, 128, 14, 14]               0\n",
            "      BatchNorm2d-28          [-1, 128, 14, 14]             256\n",
            "          Dropout-29          [-1, 128, 14, 14]               0\n",
            "           Conv2d-30           [-1, 16, 14, 14]           2,048\n",
            "           Conv2d-31           [-1, 32, 14, 14]           4,608\n",
            "             ReLU-32           [-1, 32, 14, 14]               0\n",
            "      BatchNorm2d-33           [-1, 32, 14, 14]              64\n",
            "          Dropout-34           [-1, 32, 14, 14]               0\n",
            "           Conv2d-35             [-1, 64, 7, 7]          18,432\n",
            "             ReLU-36             [-1, 64, 7, 7]               0\n",
            "      BatchNorm2d-37             [-1, 64, 7, 7]             128\n",
            "          Dropout-38             [-1, 64, 7, 7]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Conv2d-40             [-1, 10, 1, 1]             640\n",
            "================================================================\n",
            "Total params: 222,864\n",
            "Trainable params: 222,864\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.86\n",
            "Params size (MB): 0.85\n",
            "Estimated Total Size (MB): 5.72\n",
            "----------------------------------------------------------------\n",
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.4654209613800049 Batch_id=390 Accuracy=42.44: 100%|██████████| 391/391 [00:24<00:00, 16.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2918, Accuracy: 5180/10000 (51.80%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1335591077804565 Batch_id=390 Accuracy=59.42: 100%|██████████| 391/391 [00:23<00:00, 16.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0987, Accuracy: 6044/10000 (60.44%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8952425718307495 Batch_id=390 Accuracy=65.82: 100%|██████████| 391/391 [00:24<00:00, 16.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9399, Accuracy: 6624/10000 (66.24%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1296963691711426 Batch_id=390 Accuracy=70.37: 100%|██████████| 391/391 [00:24<00:00, 16.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7931, Accuracy: 7258/10000 (72.58%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6174470782279968 Batch_id=390 Accuracy=73.68: 100%|██████████| 391/391 [00:24<00:00, 16.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7296, Accuracy: 7406/10000 (74.06%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6244825124740601 Batch_id=390 Accuracy=75.98: 100%|██████████| 391/391 [00:23<00:00, 16.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6859, Accuracy: 7658/10000 (76.58%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6385086178779602 Batch_id=390 Accuracy=77.78: 100%|██████████| 391/391 [00:23<00:00, 16.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6440, Accuracy: 7754/10000 (77.54%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.597440779209137 Batch_id=390 Accuracy=78.95: 100%|██████████| 391/391 [00:22<00:00, 17.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.5998, Accuracy: 7948/10000 (79.48%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5482750535011292 Batch_id=390 Accuracy=79.97: 100%|██████████| 391/391 [00:23<00:00, 16.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.5654, Accuracy: 8058/10000 (80.58%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.4579469561576843 Batch_id=390 Accuracy=81.18: 100%|██████████| 391/391 [00:22<00:00, 17.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.5622, Accuracy: 8054/10000 (80.54%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n"
          ]
        }
      ]
    }
  ]
}