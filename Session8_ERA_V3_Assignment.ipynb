{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq"
      },
      "source": [
        "# Let's Train and test our model\n",
        "\n",
        "This time let's add a scheduler for out LR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjO3RK9UEnvF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "dropout_value = 0.05\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(\n",
        "            in_channels, in_channels, kernel_size, stride, padding, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 32 , receptive_field = 3\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 32 , receptive_field = 5\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 7\n",
        "\n",
        "            DepthwiseSeparableConv(in_channels=64, out_channels=128)\n",
        "            # output_size = 16 , receptive_field = 11\n",
        "        )\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.transition1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 16 , receptive_field = 15\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 17\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 21\n",
        "\n",
        "            DepthwiseSeparableConv(in_channels=64, out_channels=128)\n",
        "            # output_size = 16 , receptive_field = 25\n",
        "        )\n",
        "\n",
        "        # TRANSITION BLOCK 2\n",
        "        self.transition2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 16 , receptive_field = 29\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 33\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 35\n",
        "\n",
        "            DepthwiseSeparableConv(in_channels=64, out_channels=128)\n",
        "            # output_size = 16 , receptive_field = 39\n",
        "        )\n",
        "\n",
        "        # TRANSITION BLOCK 3\n",
        "        self.transition3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 16 , receptive_field = 43\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 16 , receptive_field = 47\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), dilation=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(dropout_value),\n",
        "            # output_size = 12 , receptive_field = 55\n",
        "\n",
        "            DepthwiseSeparableConv(in_channels=64, out_channels=128)\n",
        "            # output_size = 12 , receptive_field = 63\n",
        "        )\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=12)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.transition1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.transition2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.transition3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten after GAP\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# CIFAR-10 Mean and Std (calculated over the entire dataset)\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "# Albumentations Transformations\n",
        "class AlbumentationsTransform:\n",
        "    def __init__(self, mean, std):\n",
        "        self.transform = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),  # Random horizontal flip\n",
        "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),  # Shift, Scale, Rotate\n",
        "            A.CoarseDropout(max_holes=1, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16,\n",
        "                            fill_value=(np.array(mean) * 255).tolist(), mask_fill_value=None, p=0.5),  # CoarseDropout\n",
        "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0),  # Normalize\n",
        "            ToTensorV2(),  # Convert to tensor\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Albumentations expects the image in OpenCV format (HWC, uint8)\n",
        "        image = np.array(img)  # Convert PIL image to numpy array\n",
        "        return self.transform(image=image)[\"image\"]\n",
        "\n",
        "# Apply Albumentations Transformations to CIFAR-10\n",
        "train = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=AlbumentationsTransform(mean=CIFAR10_MEAN, std=CIFAR10_STD)\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=CIFAR10_MEAN, std=CIFAR10_STD)\n",
        "])\n",
        "\n",
        "test = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1,cooldown=5)\n",
        "\n",
        "\n",
        "EPOCHS = 35\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step(test_losses[-1])\n",
        "    print(f\"Learning Rate = {optimizer.param_groups[0]['lr']}\\n\")\n",
        "    #if (test_acc[epoch] > 99.4):\n",
        "    #  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoq1J_OoY_Gv",
        "outputId": "1237e961-c765-44a8-fe81-768aa58fc087"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "CUDA Available? True\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
            "           Dropout-4           [-1, 16, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,608\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 32, 32, 32]              64\n",
            "           Dropout-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,432\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
            "          Dropout-12           [-1, 64, 16, 16]               0\n",
            "           Conv2d-13           [-1, 64, 16, 16]             640\n",
            "           Conv2d-14          [-1, 128, 16, 16]           8,320\n",
            "             ReLU-15          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "          Dropout-17          [-1, 128, 16, 16]               0\n",
            "DepthwiseSeparableConv-18          [-1, 128, 16, 16]               0\n",
            "           Conv2d-19           [-1, 16, 16, 16]           2,048\n",
            "           Conv2d-20           [-1, 32, 16, 16]           4,608\n",
            "             ReLU-21           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
            "          Dropout-23           [-1, 32, 16, 16]               0\n",
            "           Conv2d-24           [-1, 64, 16, 16]          18,432\n",
            "             ReLU-25           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-26           [-1, 64, 16, 16]             128\n",
            "          Dropout-27           [-1, 64, 16, 16]               0\n",
            "           Conv2d-28           [-1, 64, 16, 16]             640\n",
            "           Conv2d-29          [-1, 128, 16, 16]           8,320\n",
            "             ReLU-30          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
            "          Dropout-32          [-1, 128, 16, 16]               0\n",
            "DepthwiseSeparableConv-33          [-1, 128, 16, 16]               0\n",
            "           Conv2d-34           [-1, 16, 16, 16]           2,048\n",
            "           Conv2d-35           [-1, 32, 16, 16]           4,608\n",
            "             ReLU-36           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-37           [-1, 32, 16, 16]              64\n",
            "          Dropout-38           [-1, 32, 16, 16]               0\n",
            "           Conv2d-39           [-1, 64, 16, 16]          18,432\n",
            "             ReLU-40           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "          Dropout-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]             640\n",
            "           Conv2d-44          [-1, 128, 16, 16]           8,320\n",
            "             ReLU-45          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
            "          Dropout-47          [-1, 128, 16, 16]               0\n",
            "DepthwiseSeparableConv-48          [-1, 128, 16, 16]               0\n",
            "           Conv2d-49           [-1, 16, 16, 16]           2,048\n",
            "           Conv2d-50           [-1, 32, 16, 16]           4,608\n",
            "             ReLU-51           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "          Dropout-53           [-1, 32, 16, 16]               0\n",
            "           Conv2d-54           [-1, 64, 14, 14]          18,432\n",
            "             ReLU-55           [-1, 64, 14, 14]               0\n",
            "      BatchNorm2d-56           [-1, 64, 14, 14]             128\n",
            "          Dropout-57           [-1, 64, 14, 14]               0\n",
            "           Conv2d-58           [-1, 64, 14, 14]             640\n",
            "           Conv2d-59          [-1, 128, 14, 14]           8,320\n",
            "             ReLU-60          [-1, 128, 14, 14]               0\n",
            "      BatchNorm2d-61          [-1, 128, 14, 14]             256\n",
            "          Dropout-62          [-1, 128, 14, 14]               0\n",
            "DepthwiseSeparableConv-63          [-1, 128, 14, 14]               0\n",
            "        AvgPool2d-64            [-1, 128, 1, 1]               0\n",
            "           Linear-65                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 137,690\n",
            "Trainable params: 137,690\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 9.41\n",
            "Params size (MB): 0.53\n",
            "Estimated Total Size (MB): 9.94\n",
            "----------------------------------------------------------------\n",
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.5671484470367432 Batch_id=390 Accuracy=36.39: 100%|██████████| 391/391 [00:31<00:00, 12.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 4778/10000 (47.78%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.217728614807129 Batch_id=390 Accuracy=49.14: 100%|██████████| 391/391 [00:30<00:00, 12.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 5860/10000 (58.60%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1368730068206787 Batch_id=390 Accuracy=55.52: 100%|██████████| 391/391 [00:32<00:00, 12.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 6246/10000 (62.46%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.0424096584320068 Batch_id=390 Accuracy=59.51: 100%|██████████| 391/391 [00:30<00:00, 12.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 6716/10000 (67.16%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2375929355621338 Batch_id=390 Accuracy=62.40: 100%|██████████| 391/391 [00:31<00:00, 12.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0068, Accuracy: 6968/10000 (69.68%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.954128623008728 Batch_id=390 Accuracy=64.49: 100%|██████████| 391/391 [00:31<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0066, Accuracy: 7072/10000 (70.72%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9377188682556152 Batch_id=390 Accuracy=66.27: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0060, Accuracy: 7341/10000 (73.41%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.762121856212616 Batch_id=390 Accuracy=67.76: 100%|██████████| 391/391 [00:31<00:00, 12.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0060, Accuracy: 7371/10000 (73.71%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7762648463249207 Batch_id=390 Accuracy=69.09: 100%|██████████| 391/391 [00:31<00:00, 12.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0055, Accuracy: 7646/10000 (76.46%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7751310467720032 Batch_id=390 Accuracy=69.82: 100%|██████████| 391/391 [00:31<00:00, 12.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0051, Accuracy: 7736/10000 (77.36%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6914436221122742 Batch_id=390 Accuracy=70.24: 100%|██████████| 391/391 [00:35<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0053, Accuracy: 7707/10000 (77.07%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8056640625 Batch_id=390 Accuracy=71.67: 100%|██████████| 391/391 [00:31<00:00, 12.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0051, Accuracy: 7793/10000 (77.93%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9273383021354675 Batch_id=390 Accuracy=72.20: 100%|██████████| 391/391 [00:31<00:00, 12.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0050, Accuracy: 7840/10000 (78.40%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8041922450065613 Batch_id=390 Accuracy=73.22: 100%|██████████| 391/391 [00:32<00:00, 12.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0048, Accuracy: 7931/10000 (79.31%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6137233972549438 Batch_id=390 Accuracy=73.61: 100%|██████████| 391/391 [00:33<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0047, Accuracy: 7970/10000 (79.70%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9143632650375366 Batch_id=390 Accuracy=74.20: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0047, Accuracy: 7993/10000 (79.93%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7836505174636841 Batch_id=390 Accuracy=74.60: 100%|██████████| 391/391 [00:31<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0047, Accuracy: 7988/10000 (79.88%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5944662094116211 Batch_id=390 Accuracy=74.86: 100%|██████████| 391/391 [00:31<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 8103/10000 (81.03%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7383915185928345 Batch_id=390 Accuracy=75.42: 100%|██████████| 391/391 [00:31<00:00, 12.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0042, Accuracy: 8190/10000 (81.90%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7411434650421143 Batch_id=390 Accuracy=75.46: 100%|██████████| 391/391 [00:32<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0041, Accuracy: 8223/10000 (82.23%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6207647323608398 Batch_id=390 Accuracy=76.09: 100%|██████████| 391/391 [00:30<00:00, 12.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 8108/10000 (81.08%)\n",
            "\n",
            "Learning Rate = 0.01\n",
            "\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8034059405326843 Batch_id=390 Accuracy=76.57: 100%|██████████| 391/391 [00:30<00:00, 12.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0042, Accuracy: 8211/10000 (82.11%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7415013909339905 Batch_id=390 Accuracy=78.92: 100%|██████████| 391/391 [00:30<00:00, 12.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0036, Accuracy: 8434/10000 (84.34%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7515274286270142 Batch_id=390 Accuracy=79.04: 100%|██████████| 391/391 [00:32<00:00, 12.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0036, Accuracy: 8430/10000 (84.30%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6697172522544861 Batch_id=390 Accuracy=79.04: 100%|██████████| 391/391 [00:30<00:00, 12.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8465/10000 (84.65%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6671388745307922 Batch_id=390 Accuracy=79.31: 100%|██████████| 391/391 [00:30<00:00, 12.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5406225919723511 Batch_id=390 Accuracy=79.50: 100%|██████████| 391/391 [00:30<00:00, 12.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8481/10000 (84.81%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6047564744949341 Batch_id=390 Accuracy=79.73: 100%|██████████| 391/391 [00:32<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8490/10000 (84.90%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7221367359161377 Batch_id=390 Accuracy=79.47: 100%|██████████| 391/391 [00:30<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7506778240203857 Batch_id=390 Accuracy=79.53: 100%|██████████| 391/391 [00:30<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8491/10000 (84.91%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5372615456581116 Batch_id=390 Accuracy=79.70: 100%|██████████| 391/391 [00:30<00:00, 12.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8505/10000 (85.05%)\n",
            "\n",
            "Learning Rate = 0.001\n",
            "\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6778744459152222 Batch_id=390 Accuracy=79.72: 100%|██████████| 391/391 [00:30<00:00, 12.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8504/10000 (85.04%)\n",
            "\n",
            "Learning Rate = 0.0001\n",
            "\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7876355051994324 Batch_id=390 Accuracy=80.13: 100%|██████████| 391/391 [00:32<00:00, 12.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8498/10000 (84.98%)\n",
            "\n",
            "Learning Rate = 0.0001\n",
            "\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6537963151931763 Batch_id=390 Accuracy=80.43: 100%|██████████| 391/391 [00:30<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0034, Accuracy: 8502/10000 (85.02%)\n",
            "\n",
            "Learning Rate = 0.0001\n",
            "\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.48237496614456177 Batch_id=390 Accuracy=80.37: 100%|██████████| 391/391 [00:30<00:00, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 8513/10000 (85.13%)\n",
            "\n",
            "Learning Rate = 0.0001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"model.pth\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3T8642bAtqrx",
        "outputId": "3fc1748c-c2e1-4891-c82a-c7651c81e60a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8dbe2718-cdbf-4598-b1a9-03383023906a\", \"model.pth\", 600371)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}